{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 Machine Learning\n",
    "\n",
    "This assignment will contain 4 questions with details as below. The due date is February 28st (Friday), 2020 23:59PM. Each late day will result in 20% loss of total points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42) ## to make this notebook's output stable across runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1  (20 points) Make a plan before running your model\n",
    "\n",
    "Nowadays mobile subscriptions worldwide already saturate the wireless market. In parallel, deregulation opened up markets to multiple entrants supporting competition. A large telecommunication decides to perform digital transformation to build digital competence in the new industry. One of the main challenges they face is to keep increasing their revenues from the subscription mobile services, known as the **subscriber churn**. \n",
    "\n",
    "Churn rates measure the proportion of subscribers discontinuing service during a certain period of time. As reported\n",
    "by wireless carriers across the world, average monthly churn rates vary between 1.5% and 5%. In other words,\n",
    "wireless carriers can lose 20% of their subscriber base every year, which poses significant challenges for profitability and growth. Subscriber churn may represent significant economic loss. This loss can be estimated by multiplying the average cost to acquire a new subscriber by the number of subscribers that churn. With average acquisition costs reaching $600 per subscriber churn may cost the wireless industry billions of dollars every year. On the other hand, keeping an existing subscriber is generally much cheaper and easier. Research shows that acquiring a new subscriber can be five times harder than retaining an existing one because the latter is less sensitive to both price and sales referrals. Meanwhile, improving subscriber retention can help wireless carriers increase margins. Therefore, effective subscriber churn management becomes a priority for many telecom managers as to ensure the sustainable growth of their companies.\n",
    "\n",
    "As a manager in this telecom operator, your job is to define churn manangement to be part of the digital transformation process, i.e. using automation operations to reduce subscriber churn. Below you need to write a proposal (~500 words) that will present to CEO, from business problem to machine learning problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pierfrancesco Gigli proposal:\n",
    "\n",
    "Machine Learning generally has proven historically to provide useful insights, in this case given the none complexity of acquiring the data for any mobile carrier and the dimension of those which will be necessarily big enough to train ML algorithm I suggest to conduct a preliminary analysis in order to individuate the benefits of ML to the business. Furthermore, the goal is to predict the possibility of \"churn\" and identify the loyal customers. I expect to source the data from an internal database of the company, which contains information such as, minutes of phone call, 4GB internet traffic usage, region/country of origin, the call from customer services (for complains or issues), the price of the plan and if extra cost was necessary to cover the shortage of GB from the clients etc. Once those data are cleaned/transformed, a preliminary data analysis (perhaps throughout visualization) will be conducted. This step is crucial in order to understand if the data gathered are relevant for the business insights proposed. Next, it is necessary to train the model/models and formulate the success rate in order to optimize the parameters. Since \"churn\" is a classification model, the logistic regression seems to provide the best performing model. This algorithm predicts the group to which the object being observed belongs between churned or continuing customer.\n",
    "Two model of Logistic regression can be tested, linear and polynomial. \n",
    "\n",
    "The computational steps are the following:\n",
    "* initialize vector $\\begin{equation} \\theta \\end{equation}$ with random values;\n",
    "* compute the hypothesis for the training set, **h<sub>$\\begin{equation} \\theta \\end{equation}$</sub>(x<sub>i</sub>)** ;\n",
    "* formulate and calculate the cost of the model _**J($\\begin{equation} \\theta \\end{equation}$)**_ - since the logistic regression is used, probably the cost function is a combination between two sub cost fuctions (one for churned, the other one for continuing customer); \n",
    "* optimize the value of $\\begin{equation} \\theta \\end{equation}$ so to MIN the cost function and calculate the hypothesis for the validation set;\n",
    "* compare the labels from the training set and compute the F1-score, as well revaledate the model on the test set;\n",
    "* additionally, plot the ROC curve.\n",
    "\n",
    "In order to assess if the categories are predicted correctly the F1 score is a good way to summarize the evaluation of performance in a single number. It combines precision and recall into one measure. Higher F1 score is better. The ideal F1 score value is 1. The worst value is 0. Perhaps, one of the pitfalls is that F-score is a weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account:\n",
    "\n",
    "<img src=https://cdn-images-1.medium.com/max/600/0*iokMsMqIkiumlk4J.jpg width=\"350\">\n",
    "\n",
    "Moreover, it would be helpfull to leave an unseen test set in order to asses how the model works for a virgin sample. So a k-fold cross validation will be perform on a pre-split training set, then F1-score will be asses on the untouched data set. Indeed, the object of the business model is to reduce/wave down the probability of a customer to churn. \n",
    "\n",
    "So now the carrier has a powerful tool which slice and dice usersâ€™ insights across various dimensions. For example, the company can analyze the municipality/region that has the highest concentration of customers who are likely to churn, as well the reason of customers attrition. Such important information will help the management to deploy strategies to retain clients and reach them before they decide to change carrier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (30 points) Normal Equations and Gradient Descent\n",
    "\n",
    "You have a dataset with 1,000,000 data points and 10,00 features that are generated from the following equation:\n",
    "\n",
    "\n",
    "X ~ N(0, 1) \n",
    "\n",
    "y(X) = X[:, 0] + 2 * X[:, 1] - 2 * X[:, 2] - 1.5 * X[:, 3]\n",
    "\n",
    "As you can see, only the first 4 features are informative. The remaining features are useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_sparse_uncorrelated\n",
    "\n",
    "X, y = make_sparse_uncorrelated(n_samples=10000, n_features=1000, random_state=42) \n",
    "#Prof Qiwei reduced to 10k n_samples and 1k n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1 (15 points) Implement the closed-form solution yourself and show the estimated model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\theta} = (X^T\\cdot X)^{-1}\\cdot X^T\\cdot y\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X_b = np.c_[np.ones((10000, 1)), X]  ## horizontal concatenation for each level of matrix, x_0 = 1\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) ## Compute the (multiplicative) inverse of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The theta estimated parameters for the first 4 informative features:\n",
      " [ 0.00601422  0.99562897  2.01302338 -1.99906788 -1.49087462]\n"
     ]
    }
   ],
   "source": [
    "print( \"The theta estimated parameters for the first 4 informative features:\\n\", theta_best[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n",
      "The theta estimated parameters are the following:\n",
      " [0.00601422 0.99562897 2.01302338 ... 0.00722763 0.00975365 0.0141997 ]\n"
     ]
    }
   ],
   "source": [
    "print(len(theta_best))\n",
    "print( \"The theta estimated parameters are the following:\\n\", theta_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2 (15 points) Implement the stochastic gradient descent yourself and show the estimated model parameters after 1000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: 10000\n"
     ]
    }
   ],
   "source": [
    "theta_path_SGD=[]\n",
    "m = len(X_b)\n",
    "print(\"m:\",m)\n",
    "n_epochs = 1000\n",
    "\n",
    "t0, t1 = 1, 100  # learning schedule hyperparameters # 5, 50\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.random.randn(1001,1)\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi) \n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        #theta_path_SGD.append(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#theta_path_SGD # please run this cell if u want to see the theta path code,\n",
    "#because whenever I use the function append in the loop above, my PC crash. Thanks !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.14617473],\n",
       "       [-4.04064162],\n",
       "       [-0.5335921 ],\n",
       "       ...,\n",
       "       [-2.20499994],\n",
       "       [-0.54312253],\n",
       "       [-0.6431237 ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (50 points) Zestimate this house\n",
    "\n",
    "Purchasing a house is a very big decision for most of us. Companies such as Zillows collected tons of data regarding the listing and sold price of American houses and build the predictive model, named *Zestimate*. You are expected to build a model similar as Zestimate to predict house price in California. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_california_housing()\n",
    "X_full, y_full, feature_name = dataset.data, dataset.target, dataset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1 (10 points) \n",
    "Create train and test set, each contains 80% and 20% of the dataset, respectively using *train_test_split* function in scikit-learn. Train a linear model on the train set and test on the test set, report the training error and test errors, respectively (as mean squared error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-37.023277706063915,\n",
       " array([ 4.48674910e-01,  9.72425752e-03, -1.23323343e-01,  7.83144907e-01,\n",
       "        -2.02962058e-06, -3.52631849e-03, -4.19792487e-01, -4.33708065e-01]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "lin_reg.intercept_, lin_reg.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (train)\n",
      " 0.5179331255246699\n",
      "MSE (test)\n",
      " 0.5558915986952425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred_train = lin_reg.predict(X_train)\n",
    "y_pred_test = lin_reg.predict(X_test)\n",
    "\n",
    "mse_test_train = mean_squared_error(y_train, y_pred_train)\n",
    "mse_test_test = mean_squared_error(y_test, y_pred_test)\n",
    "print(\"MSE (train)\\n\", mse_test_train)\n",
    "print(\"MSE (test)\\n\", mse_test_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2 (10 points)\n",
    "\n",
    "Perform a 10-fold cross-validation on the train set and show the mean sqaured error on test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=https://miro.medium.com/max/3115/1*me-aJdjnt3ivwAurYkB7PA.png width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4691205505092365, 0.570232780587929, 0.5226799719343237, 0.4831998511032339, 0.5430453624643138, 0.4987997733691099, 0.47454501014493755, 0.5428326667813216, 0.5413071192592376, 0.5505925614913966]\n",
      "Mean of MSE 10-fold cross-validation: \n",
      " 0.519635564764504\n",
      "MSE on unseen test set:\n",
      " 0.5572027379114174\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "MSE_10K_fold=[]\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "\n",
    "        ## separate in 10 folds TEST and TRAIN sets\n",
    "        ## fit the linear regression on Train set\n",
    "        lin_reg.fit(X_train[train_index],  y_train[train_index])\n",
    "        ## predict the y from X_test\n",
    "        y_pred = lin_reg.predict( X_train[test_index])\n",
    "        ## Mean Squared Error between test and predictions\n",
    "        mse = mean_squared_error(y_train[test_index], y_pred)\n",
    "        MSE_10K_fold.append(mse)\n",
    "        \n",
    "print(MSE_10K_fold)\n",
    "\n",
    "print(\"Mean of MSE 10-fold cross-validation: \\n\", np.mean(MSE_10K_fold) )\n",
    "\n",
    "# MSE on unseen test set\n",
    "y_pred = lin_reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE on unseen test set:\\n\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3 (10 points) \n",
    " \n",
    "Add 2-degree polynomial features (with no interactions) and perform 10-fold cross-validation on the train set. Show the mean squared error on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4507645145708705, 0.5497150016279139, 0.48916176019417323, 0.4672497242624254, 0.5226894775594849, 8.06904981313106, 0.46878466468243424, 0.5273686863819486, 0.5299070780792456, 0.5277708669962561]\n",
      "Mean of MSE (on test) 10-fold cross-validation on 2^degree polynomial: \n",
      " 1.2602461587485814\n",
      "MSE on unseen test set:\n",
      " 0.8169378605699771\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "## polynomial features (with no interaction) :\n",
    "## poly_features = sklearn.preprocessing.PolynomialFeatures(degree=2, interaction_only=False)\n",
    "## poly_features.fit_transform(X_full)  \n",
    "X_new_full=np.hstack((X_full, X_full**2))\n",
    "\n",
    "X_train_poly, X_test_poly, y_train, y_test = train_test_split(X_new_full, y_full, test_size=0.2, random_state=42)\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "MSE_10K_fold_2_poly=[]\n",
    "for train_index, test_index in kf.split(X_train_poly):\n",
    "        ## separate in 10 folds TEST and TRAIN sets\n",
    "        ## fit the linear regression on Train set\n",
    "        lin_reg.fit(X_train_poly[train_index],  y_train[train_index])\n",
    "        ## predict the y from X_test\n",
    "        y_pred = lin_reg.predict(X_train_poly[test_index])\n",
    "        ## Mean Squared Error on TEST !\n",
    "        mse = mean_squared_error(y_train[test_index], y_pred)\n",
    "        MSE_10K_fold_2_poly.append(mse)\n",
    "        \n",
    "print(MSE_10K_fold_2_poly)\n",
    "\n",
    "print(\"Mean of MSE (on test) 10-fold cross-validation on 2^degree polynomial: \\n\", np.mean(MSE_10K_fold_2_poly) )\n",
    "\n",
    "# MSE on unseen test set\n",
    "y_pred = lin_reg.predict(X_test_poly)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE on unseen test set:\\n\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.4 (20 points)\n",
    "\n",
    "Use ridge and lasso regression on different feature combinations (linear features vs. polynomial features) and compare with linear regression. Explain which method works better in this case\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3446052311240813 [ 1.48196324e-01  5.72821070e-03  0.00000000e+00 -0.00000000e+00\n",
      " -8.16437293e-06 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00]\n",
      "MSE Lasso with Linear model: \n",
      " 0.9380337514945428\n"
     ]
    }
   ],
   "source": [
    "## LASSO\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "L_reg=Lasso(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, \n",
    "      copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, \n",
    "      random_state=None, selection='cyclic')\n",
    "\n",
    "L_reg.fit(X_train, y_train)\n",
    "print(L_reg.intercept_, L_reg.coef_)\n",
    "\n",
    "y_pred_Lasso = L_reg.predict(X_test)\n",
    "print(\"MSE Lasso with Linear model: \\n\",mean_squared_error(y_pred_Lasso, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Lasso with Linear model: \n",
      " 0.9380337514945428 alpha 1.0\n",
      "MSE Lasso with Linear model: \n",
      " 0.6135115198058131 alpha 0.1\n",
      "MSE Lasso with Linear model: \n",
      " 0.5444491581246518 alpha 0.01\n"
     ]
    }
   ],
   "source": [
    "## LOOP ALPHA LASSO\n",
    "\n",
    "for i in [1.0,0.1,0.01]:\n",
    "    L_reg=Lasso(alpha=i, fit_intercept=True, normalize=False, precompute=False, \n",
    "      copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, \n",
    "      random_state=None, selection='cyclic')\n",
    "\n",
    "    L_reg.fit(X_train, y_train)\n",
    "    #print(L_reg.intercept_, L_reg.coef_)\n",
    "\n",
    "    y_pred_Lasso = L_reg.predict(X_test)\n",
    "    print(\"MSE Lasso with Linear model: \\n\", mean_squared_error(y_pred_Lasso, y_test), \"alpha\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-37.01941983801211 [ 4.48510924e-01  9.72596535e-03 -1.23014157e-01  7.81416761e-01\n",
      " -2.02581346e-06 -3.52585878e-03 -4.19786908e-01 -4.33680793e-01]\n",
      "MSE Ridge with Linear model: \n",
      " 0.5558034669932189\n"
     ]
    }
   ],
   "source": [
    "## RIDGE\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "R_reg=Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, \n",
    "      max_iter=None, tol=0.001, solver='auto', random_state=None)\n",
    "\n",
    "R_reg.fit(X_train, y_train)\n",
    "print(R_reg.intercept_, R_reg.coef_)\n",
    "\n",
    "y_pred_Ridge = R_reg.predict(X_test)\n",
    "print(\"MSE Ridge with Linear model: \\n\", mean_squared_error(y_pred_Ridge, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Ridge with Linear model: \n",
      " 0.5558034669932189 alpha 1.0\n",
      "MSE Ridge with Linear model: \n",
      " 0.5558827543113781 alpha 0.1\n",
      "MSE Ridge with Linear model: \n",
      " 0.5558907139437501 alpha 0.01\n"
     ]
    }
   ],
   "source": [
    "## LOOP ALPHA RIDGE\n",
    "\n",
    "for i in [1.0,0.1,0.01]:\n",
    "    R_reg=Ridge(alpha=i, fit_intercept=True, normalize=False, copy_X=True, \n",
    "      max_iter=None, tol=0.001, solver='auto', random_state=None)\n",
    "\n",
    "    R_reg.fit(X_train, y_train)\n",
    "    #print(R_reg.intercept_, R_reg.coef_)\n",
    "\n",
    "    y_pred_Ridge = R_reg.predict(X_test)\n",
    "    print(\"MSE Ridge with Linear model: \\n\", mean_squared_error(y_pred_Ridge, y_test), \"alpha\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_full_poly=np.hstack((X_full, X_full**2)) ## 2^ degree poly\n",
    "X_train_poly, X_test_poly, y_train, y_test = train_test_split(X_full_poly, y_full, test_size=0.2, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-17.60697156430524 [ 4.96833620e-01 -3.25520312e-03 -7.09195968e-02  4.36013141e-01\n",
      " -8.29534462e-06 -1.20163753e-02 -0.00000000e+00 -0.00000000e+00\n",
      " -7.65256345e-03  2.32818016e-04  5.19296047e-04 -9.55823144e-03\n",
      "  1.90553274e-10  8.55370108e-06 -5.69603033e-03  1.74576546e-03]\n",
      "MSE Lasso with polynomial model: \n",
      " 0.5304171944111588\n"
     ]
    }
   ],
   "source": [
    "## LASSO Poly\n",
    "L_reg.fit(X_train_poly, y_train)\n",
    "print(L_reg.intercept_, L_reg.coef_)\n",
    "y_pred_Lasso_poly = L_reg.predict(X_test_poly)\n",
    "print(\"MSE Lasso with polynomial model: \\n\",mean_squared_error(y_pred_Lasso_poly, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Lasso with polynomial model: \n",
      " 0.6222391950210286 alpha 1.0\n",
      "MSE Lasso with polynomial model: \n",
      " 0.5644431894431977 alpha 0.1\n",
      "MSE Lasso with polynomial model: \n",
      " 0.5304171944111588 alpha 0.01\n"
     ]
    }
   ],
   "source": [
    "## LOOP ALPHA LASSO poly\n",
    "\n",
    "for i in [1.0,0.1,0.01]:\n",
    "    L_reg=Lasso(alpha=i, fit_intercept=True, normalize=False, precompute=False, \n",
    "      copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, \n",
    "      random_state=None, selection='cyclic')\n",
    "    L_reg.fit(X_train_poly, y_train)\n",
    "    #print(L_reg.intercept_, L_reg.coef_)\n",
    "    y_pred_Lasso_poly = L_reg.predict(X_test_poly)\n",
    "    print(\"MSE Lasso with polynomial model: \\n\", mean_squared_error(y_pred_Lasso_poly, y_test), \"alpha\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-244.33647845025564 [ 6.32208462e-01 -3.92338358e-03 -2.16781330e-01  1.44646646e+00\n",
      " -6.11875114e-06 -1.20936978e-02 -1.83847050e+00 -4.30908485e+00\n",
      " -1.38366725e-02  2.56217916e-04  3.10877507e-03 -9.60632868e-02\n",
      "  2.41773247e-10  8.28180047e-06  1.96713832e-02 -1.61568577e-02]\n",
      "MSE Ridge with polynomial model: \n",
      " 0.841974220953937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pierfrancesco\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=5.23688e-19): result may not be accurate.\n",
      "  overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "## RIDGE Poly\n",
    "R_reg.fit(X_train_poly, y_train)\n",
    "print(R_reg.intercept_, R_reg.coef_)\n",
    "y_pred_Ridge_poly = R_reg.predict(X_test_poly)\n",
    "print(\"MSE Ridge with polynomial model: \\n\", mean_squared_error(y_pred_Ridge_poly, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Ridge with polynomial model: \n",
      " 0.8358038797191253 alpha 1.0\n",
      "MSE Ridge with polynomial model: \n",
      " 0.841324864811901 alpha 0.1\n",
      "MSE Ridge with polynomial model: \n",
      " 0.841974220953937 alpha 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pierfrancesco\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=7.91907e-19): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "C:\\Users\\Pierfrancesco\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=5.48015e-19): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "C:\\Users\\Pierfrancesco\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=5.23688e-19): result may not be accurate.\n",
      "  overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "## LOOP ALPHA RIDGE poly\n",
    "for i in [1.0,0.1,0.01]:\n",
    "    R_reg=Ridge(alpha=i, fit_intercept=True, normalize=False, copy_X=True, \n",
    "      max_iter=None, tol=0.001, solver='auto', random_state=None)\n",
    "    R_reg.fit(X_train_poly, y_train)\n",
    "    #print(R_reg.intercept_, R_reg.coef_)\n",
    "    y_pred_Ridge_poly = R_reg.predict(X_test_poly)\n",
    "    print(\"MSE Ridge with polynomial model: \\n\", mean_squared_error(y_pred_Ridge_poly, y_test), \"alpha\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso L<sub>1</sub> as got a MSE lower (thus more accurate) than the Ridge L<sub>2</sub> in the poly features transformation, while it is higher in the linear regression. Moreover, in both cases (linear and poly) by reducing the alpha in the Ridge regressions, it does not change/reduce the MSE as strong as in the Lasso regression. The motivation behind it could be reconduct to the fact that Lasso differently than Ridge penalize the irrelevant features by reduce them exactly to zero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
